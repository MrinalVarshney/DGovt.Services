{"ast":null,"code":"import { createCp } from './cp.js';\nimport { createFlush } from './flush.js';\nimport { createLs } from './ls.js';\nimport { createMkdir } from './mkdir.js';\nimport { createMv } from './mv.js';\nimport { createRead } from './read.js';\nimport { createRm } from './rm.js';\nimport { createStat } from './stat.js';\nimport { createWrite } from './write.js';\nexport function createFiles(client) {\n  return {\n    cp: createCp(client),\n    flush: createFlush(client),\n    ls: createLs(client),\n    mkdir: createMkdir(client),\n    mv: createMv(client),\n    read: createRead(client),\n    rm: createRm(client),\n    stat: createStat(client),\n    write: createWrite(client)\n  };\n}","map":{"version":3,"names":["createCp","createFlush","createLs","createMkdir","createMv","createRead","createRm","createStat","createWrite","createFiles","client","cp","flush","ls","mkdir","mv","read","rm","stat","write"],"sources":["/home/lokesh/Desktop/DGovt.Services/Hack36_1/client/node_modules/kubo-rpc-client/src/files/index.ts"],"sourcesContent":["import { createCp } from './cp.js'\nimport { createFlush } from './flush.js'\nimport { createLs } from './ls.js'\nimport { createMkdir } from './mkdir.js'\nimport { createMv } from './mv.js'\nimport { createRead } from './read.js'\nimport { createRm } from './rm.js'\nimport { createStat } from './stat.js'\nimport { createWrite } from './write.js'\nimport type { AddProgressFn, HTTPRPCOptions, IPFSPath } from '../index.js'\nimport type { HTTPRPCClient } from '../lib/core.js'\nimport type { Mtime, MtimeLike } from 'ipfs-unixfs'\nimport type { CID, Version } from 'multiformats/cid'\n\nexport interface MFSEntry {\n  /**\n   * The object's name\n   */\n  name: string\n\n  /**\n   * The object's type (directory or file)\n   */\n  type: 'directory' | 'file'\n\n  /**\n   * The size of the file in bytes\n   */\n  size: number\n\n  /**\n   * The CID of the object\n   */\n  cid: CID\n\n  /**\n   * The UnixFS mode as a Number\n   */\n  mode?: number\n\n  /**\n   * An object with numeric secs and nsecs properties\n   */\n  mtime?: Mtime\n}\n\nexport interface MFSOptions extends HTTPRPCOptions {\n  /**\n   * If true the changes will be immediately flushed to disk\n   */\n  flush?: boolean\n}\n\nexport interface FilesChmodOptions extends MFSOptions {\n  /**\n   * If true mode will be applied to the entire tree under path\n   */\n  recursive?: boolean\n\n  /**\n   * The hash algorithm to use for any updated entries\n   */\n  hashAlg?: string\n\n  /**\n   * The CID version to use for any updated entries\n   */\n  cidVersion?: Version\n\n  /**\n   * The threshold for splitting any modified folders into HAMT shards\n   */\n  shardSplitThreshold?: number\n}\n\nexport interface FilesCpOptions extends MFSOptions {\n  /**\n   * The value or node that was fetched during the get operation\n   */\n  parents?: boolean\n\n  /**\n   * The hash algorithm to use for any updated entries\n   */\n  hashAlg?: string\n\n  /**\n   * The CID version to use for any updated entries\n   */\n  cidVersion?: Version\n\n  /**\n   * The threshold for splitting any modified folders into HAMT shards\n   */\n  shardSplitThreshold?: number\n}\n\nexport interface FilesMkdirOptions extends MFSOptions {\n  /**\n   * If true, create intermediate directories\n   */\n  parents?: boolean\n\n  /**\n   * An integer that represents the file mode\n   */\n  mode?: number\n\n  /**\n   * A Date object, an object with { secs, nsecs } properties where secs is the number of seconds since (positive) or before (negative) the Unix Epoch began and nsecs is the number of nanoseconds since the last full second, or the output of process.hrtime()\n   */\n  mtime?: MtimeLike\n\n  /**\n   * The hash algorithm to use for any updated entries\n   */\n  hashAlg?: string\n\n  /**\n   * The CID version to use for any updated entries\n   */\n  cidVersion?: Version\n\n  /**\n   * The threshold for splitting any modified folders into HAMT shards\n   */\n  shardSplitThreshold?: number\n}\n\nexport interface FilesStatOptions extends HTTPRPCOptions {\n  /**\n   * If true, return only the CID\n   */\n  hash?: boolean\n\n  /**\n   * If true, return only the size\n   */\n  size?: boolean\n\n  /**\n   * If true, compute the amount of the DAG that is local and if possible the total size\n   */\n  withLocal?: boolean\n}\n\nexport interface FilesStatResult {\n  /**\n   * A CID instance\n   */\n  cid: CID\n\n  /**\n   * The file size in Bytes\n   */\n  size: number\n\n  /**\n   * The size of the DAGNodes making up the file in Bytes\n   */\n  cumulativeSize: number\n\n  /**\n   * Either directory or file\n   */\n  type: 'directory' | 'file'\n\n  /**\n   * If type is directory, this is the number of files in the directory. If it is file it is the number of blocks that make up the file\n   */\n  blocks: number\n\n  /**\n   * Indicates if locality information is present\n   */\n  withLocality: boolean\n\n  /**\n   * Indicates if the queried dag is fully present locally\n   */\n  local?: boolean\n\n  /**\n   * Indicates the cumulative size of the data present locally\n   */\n  sizeLocal?: number\n\n  /**\n   * UnixFS mode if applicable\n   */\n  mode?: number\n\n  /**\n   * UnixFS mtime if applicable\n   */\n  mtime?: Mtime\n}\n\nexport interface FilesTouchOptions extends MFSOptions {\n  /**\n   * A Date object, an object with { secs, nsecs } properties where secs is the number of seconds since (positive) or before (negative) the Unix Epoch began and nsecs is the number of nanoseconds since the last full second, or the output of process.hrtime()\n   */\n  mtime?: MtimeLike\n\n  /**\n   * The hash algorithm to use for any updated entries\n   */\n  hashAlg?: string\n\n  /**\n   * The CID version to use for any updated entries\n   */\n  cidVersion?: Version\n\n  /**\n   * The threshold for splitting any modified folders into HAMT shards\n   */\n  shardSplitThreshold?: number\n}\n\nexport interface FilesRmOptions extends MFSOptions {\n  /**\n   * If true all paths under the specifed path(s) will be removed\n   */\n  recursive?: boolean\n\n  /**\n   * The hash algorithm to use for any updated entries\n   */\n  hashAlg?: string\n\n  /**\n   * The CID version to use for any updated entries\n   */\n  cidVersion?: Version\n\n  /**\n   * The threshold for splitting any modified folders into HAMT shards\n   */\n  shardSplitThreshold?: number\n}\n\nexport interface FilesReadOptions extends HTTPRPCOptions {\n  /**\n   * An offset to start reading the file from\n   */\n  offset?: number\n\n  /**\n   * An optional max length to read from the file\n   */\n  length?: number\n}\n\nexport interface FilesWriteOptions extends MFSOptions {\n  /**\n   * An offset within the file to start writing at\n   */\n  offset?: number\n\n  /**\n   * Optionally limit how many bytes are written\n   */\n  length?: number\n\n  /**\n   * Create the MFS path if it does not exist\n   */\n  create?: boolean\n\n  /**\n   * Create intermediate MFS paths if they do not exist\n   */\n  parents?: boolean\n\n  /**\n   * Truncate the file at the MFS path if it would have been larger than the passed content\n   */\n  truncate?: boolean\n\n  /**\n   * If true, DAG leaves will contain raw file data and not be wrapped in a protobuf\n   */\n  rawLeaves?: boolean\n\n  /**\n   * An integer that represents the file mode\n   */\n  mode?: number\n\n  /**\n   * A Date object, an object with { secs, nsecs } properties where secs is the number of seconds since (positive) or before (negative) the Unix Epoch began and nsecs is the number of nanoseconds since the last full second, or the output of process.hrtime()\n   */\n  mtime?: MtimeLike\n\n  /**\n   * The hash algorithm to use for any updated entries\n   */\n  hashAlg?: string\n\n  /**\n   * The CID version to use for any updated entries\n   */\n  cidVersion?: Version\n\n  /**\n   * The threshold for splitting any modified folders into HAMT shards\n   */\n  shardSplitThreshold?: number\n\n  /**\n   * If writing a file and only a single leaf would be present, store the file data in the root node\n   */\n  reduceSingleLeafToSelf?: boolean\n\n  /**\n   * What sort of DAG structure to create\n   */\n  strategy?: 'balanced' | 'trickle'\n\n  /**\n   * Callback to be notified of write progress\n   */\n  progress?: AddProgressFn\n}\n\nexport interface FilesMvOptions extends MFSOptions {\n  /**\n   * Create intermediate MFS paths if they do not exist\n   */\n  parents?: boolean\n\n  /**\n   * The hash algorithm to use for any updated entries\n   */\n  hashAlg?: string\n\n  /**\n   * The CID version to use for any updated entries\n   */\n  cidVersion?: Version\n\n  /**\n   * The threshold for splitting any modified folders into HAMT shards\n   */\n  shardSplitThreshold?: number\n\n  recursive?: boolean\n}\n\nexport interface LSOptions extends HTTPRPCOptions {\n  long?: boolean\n}\n\nexport interface FilesAPI {\n  /**\n   * Copy files from one location to another\n   *\n   * - If from has multiple values then to must be a directory.\n   * - If from has a single value and to exists and is a directory, from will be copied into to.\n   * - If from has a single value and to exists and is a file, from must be a file and the contents of to will be replaced with the contents of from otherwise an error will be returned.\n   * - If from is an IPFS path, and an MFS path exists with the same name, the IPFS path will be chosen.\n   * - If from is an IPFS path and the content does not exist in your node's repo, only the root node of the source file with be retrieved from the network and linked to from the destination. The remainder of the file will be retrieved on demand.\n   *\n   * @example\n   * ```js\n   * // To copy a file\n   * await ipfs.files.cp('/src-file', '/dst-file')\n   *\n   * // To copy a directory\n   * await ipfs.files.cp('/src-dir', '/dst-dir')\n   *\n   * // To copy multiple files to a directory\n   * await ipfs.files.cp('/src-file1', '/src-file2', '/dst-dir')\n   * ```\n   */\n  cp(from: IPFSPath | IPFSPath[], to: string, options?: FilesCpOptions): Promise<void>\n\n  /**\n   * Make a directory in your MFS\n   */\n  mkdir(path: string, options?: FilesMkdirOptions): Promise<void>\n\n  /**\n   * Get file or directory statistics\n   */\n  stat(ipfsPath: IPFSPath, options?: FilesStatOptions): Promise<FilesStatResult>\n\n  /**\n   * Remove a file or directory\n   *\n   * @example\n   * ```js\n   * // To remove a file\n   * await ipfs.files.rm('/my/beautiful/file.txt')\n   *\n   * // To remove multiple files\n   * await ipfs.files.rm(['/my/beautiful/file.txt', '/my/other/file.txt'])\n   *\n   * // To remove a directory\n   * await ipfs.files.rm('/my/beautiful/directory', { recursive: true })\n   * ```\n   */\n  rm(ipfsPaths: string | string[], options?: FilesRmOptions): Promise<void>\n\n  /**\n   * Read a file\n   *\n   * @example\n   * ```js\n   * const chunks = []\n   *\n   * for await (const chunk of ipfs.files.read('/hello-world')) {\n   *   chunks.push(chunk)\n   * }\n   *\n   * console.log(uint8ArrayConcat(chunks).toString())\n   * // Hello, World!\n   * ```\n   */\n  read(ipfsPath: IPFSPath, options?: FilesReadOptions): AsyncIterable<Uint8Array>\n\n  /**\n   * Write to an MFS path\n   *\n   * @example\n   * ```js\n   * await ipfs.files.write('/hello-world', new TextEncoder().encode('Hello, world!'))\n   * ```\n   */\n  write(ipfsPath: string, content: string | Uint8Array | Blob | AsyncIterable<Uint8Array> | Iterable<Uint8Array>, options?: FilesWriteOptions): Promise<void>\n\n  /**\n   * Move files from one location to another\n   *\n   * - If from has multiple values then to must be a directory.\n   * - If from has a single value and to exists and is a directory, from will be moved into to.\n   * - If from has a single value and to exists and is a file, from must be a file and the contents of to will be replaced with the contents of from otherwise an error will be returned.\n   * - If from is an IPFS path, and an MFS path exists with the same name, the IPFS path will be chosen.\n   * - If from is an IPFS path and the content does not exist in your node's repo, only the root node of the source file with be retrieved from the network and linked to from the destination. The remainder of the file will be retrieved on demand.\n   * - All values of from will be removed after the operation is complete unless they are an IPFS path.\n   *\n   * @example\n   * ```js\n   * await ipfs.files.mv('/src-file', '/dst-file')\n   *\n   * await ipfs.files.mv('/src-dir', '/dst-dir')\n   *\n   * await ipfs.files.mv('/src-file1', '/src-file2', '/dst-dir')\n   * ```\n   */\n  mv(from: string | string[], to: string, options?: FilesMvOptions): Promise<void>\n\n  /**\n   * Flush a given path's data to the disk\n   *\n   * @example\n   * ```js\n   * const cid = await ipfs.files.flush('/')\n   * ```\n   */\n  flush(ipfsPath: string, options?: HTTPRPCOptions): Promise<CID>\n\n  /**\n   * List directories in the local mutable namespace\n   *\n   * @example\n   * ```js\n   * for await (const file of ipfs.files.ls('/screenshots')) {\n   *   console.log(file.name)\n   * }\n   * // 2018-01-22T18:08:46.775Z.png\n   * // 2018-01-22T18:08:49.184Z.png\n   * ```\n   */\n  ls(ipfsPath: IPFSPath, options?: LSOptions): AsyncIterable<MFSEntry>\n}\n\nexport function createFiles (client: HTTPRPCClient): FilesAPI {\n  return {\n    cp: createCp(client),\n    flush: createFlush(client),\n    ls: createLs(client),\n    mkdir: createMkdir(client),\n    mv: createMv(client),\n    read: createRead(client),\n    rm: createRm(client),\n    stat: createStat(client),\n    write: createWrite(client)\n  }\n}\n"],"mappings":"AAAA,SAASA,QAAQ,QAAQ,SAAS;AAClC,SAASC,WAAW,QAAQ,YAAY;AACxC,SAASC,QAAQ,QAAQ,SAAS;AAClC,SAASC,WAAW,QAAQ,YAAY;AACxC,SAASC,QAAQ,QAAQ,SAAS;AAClC,SAASC,UAAU,QAAQ,WAAW;AACtC,SAASC,QAAQ,QAAQ,SAAS;AAClC,SAASC,UAAU,QAAQ,WAAW;AACtC,SAASC,WAAW,QAAQ,YAAY;AAsdxC,OAAM,SAAUC,WAAWA,CAAEC,MAAqB;EAChD,OAAO;IACLC,EAAE,EAAEX,QAAQ,CAACU,MAAM,CAAC;IACpBE,KAAK,EAAEX,WAAW,CAACS,MAAM,CAAC;IAC1BG,EAAE,EAAEX,QAAQ,CAACQ,MAAM,CAAC;IACpBI,KAAK,EAAEX,WAAW,CAACO,MAAM,CAAC;IAC1BK,EAAE,EAAEX,QAAQ,CAACM,MAAM,CAAC;IACpBM,IAAI,EAAEX,UAAU,CAACK,MAAM,CAAC;IACxBO,EAAE,EAAEX,QAAQ,CAACI,MAAM,CAAC;IACpBQ,IAAI,EAAEX,UAAU,CAACG,MAAM,CAAC;IACxBS,KAAK,EAAEX,WAAW,CAACE,MAAM;GAC1B;AACH","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}